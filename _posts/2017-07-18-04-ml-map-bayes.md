---
layout: post
title: 最尤推定、MAP推定、ベイズ推定
tags:
- 機械学習
- 統計
use_mathjax: true
---

適当な確率空間 \\((\Omega, \Sigma, P)\\) について考えます。

最尤推定の前に、まずは一般的な推定から考えていきましょう。

## そもそも推定とは？

一般的に推定とは、独立同分布(i.i.d)な確率変数 \\(X_1, X_2, ..., X_n\\) に対し、これらの確率変数の性質を特徴づけるような値 \\(\theta\\) (平均、分散、密度関数のパラメータ、etc.)を、確率変数 \\(\hat{\theta}(X_1, X_2, ..., X_n)\\) を用いて、自然言語の意味で「推定」する操作のことを言います。

このとき、 \\(\hat{\theta}\\) は \\(\mathbb{R}^n \to \mathbb{R}\\) の関数であり、 \\(x_1, x_2, ..., x_n \in \mathbb{R}\\) に対して、 \\(\hat{\theta}(x_1, x_2, ..., x_n)\\) を推定値といい、確率変数 \\(\hat{\theta}(X_1, X_2, ..., X_n)\\) を推定量といいます。

## 最尤推定 (maximum likelihood estimation)

i.i.d.な確率変数 \\(X_1, X_2, ..., X_n\\) について、これらが特定の値を取る確率が、あるパラメータ \\(\theta\\) を取る関数 \\(f\\) によって、

\\[P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n) = f(x_1, x_2, ..., x_n; \theta)\\]

と表せると仮定します。

この \\(\theta\\) が未知であるとして、最尤推定では以下で定義する尤度関数 \\(L(\theta)\\) を最大化する \\(\theta\\) を推定値 \\(\hat{\theta}_{ML}\\) として扱います。

+ \\(L(\theta) = f(x_1, x_2, ..., x_n; \theta)\\)

すなわち、

\\[\hat{\theta}_{ML} = \mathrm{argmax} _{\theta}L(\theta)\\]

また、実用上直接 \\(L(\theta)\\) を扱わず、以下のようにすることが多いです。

\\[\hat{\theta}_{ML} = \mathrm{argmax} _{\theta}\log L(\theta)\\]

## MAP推定 (maximum a posteriori)

先ほどとほぼ同様の問題設定を行います。

唯一違うのは、今回は \\(\theta\\) が既知の確率分布に従う確率変数であると仮定します。

\\(\theta\\) がある値であるとき、 \\(X_1 = x_1, X_2 = x_2, ..., X_n = x_n\\) である確率は、以下の式で表すことができます。

\\[P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n \vert \theta) = f(x_1, x_2, ..., x_n; \theta)\\]

このとき、確率変数 \\(X_1, X_2, ..., X_n\\) に対して、 \\(X_1 = x_1, X_2 = x_2, ..., X_n = x_n\\) であった場合に、 \\(\theta\\) がある値である確率を考えます。

\\[P(\theta \vert X_1 = x_1, X_2 = x_2, ..., X_n = x_n)\\]
\\[= \frac{f(x_1, x_2, ..., x_n; \theta)P(\theta)}{P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n)}\\]
\\[\propto f(x_1, x_2, ..., x_n; \theta)P(\theta)\\]

このような確率を、事後確率といいます。(対して、 \\(P(\theta)\\) を事前確率と呼んだりします)

この事後確率を最大化するような \\(\theta\\) を推定値 \\(\hat{\theta}_{MAP}\\) として扱います。

\\[\hat{\theta}_{MAP} = \mathrm{argmax} _{\theta} f(x_1, x_2, ..., x_n; \theta)P(\theta)\\]

また、この場合についても実用上対数を取ったものを計算することが多いようです。

\\[\hat{\theta}_{MAP} = \mathrm{argmax} _{\theta} \log f(x_1, x_2, ..., x_n; \theta)P(\theta)\\]

なお、式を見ればわかりますが、MAP推定は \\(\theta\\) が一様分布である場合に最尤推定と等しくなります。

## ベイズ推定

最尤推定とMAP推定では最もそれっぽい \\(\theta\\) の値を一つ求めていましたが、ベイズ推定では \\(\theta\\) の事後分布をそのまま使います。

\\[P(\theta \vert X_1 = x_1, X_2 = x_2, ..., X_n = x_n)\\]
\\[= \frac{f(x_1, x_2, ..., x_n; \theta)P(\theta)}{P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n)}\\]

こうすることによって、「ありえたかもしれない別の可能性」についても考慮された値を考えることができるようになります。


以上では最尤推定、MAP推定、ベイズ推定について、それぞれの推定値が何を意味しているのかの説明のみを行いました。

実際にこの問題をどう解けばいいのかについては、この記事は全く触れていませんし、そもそもあまり一般的な議論ができるものではありません。この点については、確率変数の分布ごとに個別に考えていく必要があります。
